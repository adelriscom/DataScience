{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Decision Trees & Ensembles\n",
    "**Introduction**\n",
    "\n",
    ">Diabetes mellitus, often referred to as diabetes, is a prevalent chronic metabolic disorder that affects millions of individuals worldwide. It is characterized by elevated blood glucose levels, which can lead to various complications if not effectively managed. Given the growing incidence of diabetes and its associated health risks, accurate prediction and diagnosis are crucial for timely intervention and treatment.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    ">- The primary objective of this study is to develop and evaluate tree-based machine learning models for predicting whether an individual has diabetes or not, utilizing a comprehensive dataset. This dataset, a valuable resource for research and analysis, contains a range of clinical and demographic features that are associated with the likelihood of diabetes.\n",
    "\n",
    "## **Methods & Procedures:**\n",
    "\n",
    "**Data Preprocessing:**\n",
    ">- Handling missing values\n",
    ">- Dealing with data imbalance\n",
    ">- Feature scaling and encoding\n",
    "\n",
    "**Model Training:**\n",
    "\n",
    ">- Utilizing various tree-based machine learning algorithms\n",
    ">- Fine-tuning model hyperparameters for optimal performance\n",
    "\n",
    "**Evaluation Measures:**\n",
    ">- Using accuracy, precision, recall, F1-score, and the confusion matrix\n",
    ">- Assessing model generalization and robustness\n",
    "\n",
    "**Feature Selection:**\n",
    ">- Identifying the most important features for classification\n",
    "\n",
    "**Results:**\n",
    ">- The study provides a comprehensive analysis of the models' performance, including comparisons of different tree-based machine learning techniques.\n",
    "\n",
    "**Model Performance with All Features:**\n",
    ">- Evaluation of models using the entire feature set to understand their baseline performance.\n",
    "\n",
    "**Model Performance with Feature Selection:**\n",
    ">- Assessment of models after feature selection to identify the most influential attributes in diabetes prediction.\n",
    "\n",
    "**Conclusion:**\n",
    ">- In conclusion, the research aims to contribute to the field of healthcare and predictive modeling by exploring the potential of tree-based machine learning algorithms for diabetes prediction. By addressing data preprocessing, model training, evaluation, and feature selection, we seek to enhance our understanding of the effectiveness of these techniques in medical decision support. Ultimately, the findings from this study may pave the way for more accurate and efficient diabetes risk assessment, offering valuable insights for both medical professionals and individuals at risk of this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"/Users/alexanderdelriscomorales/Downloads/AI_ML_Files/diabetes.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">##### Exploratory Data Analysis\n",
    ">> It's a good practice to perform some initial exploratory data analysis to understand the dataset. Check for missing values, data types, and the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome\n",
       "0    500\n",
       "1    268\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Data types\n",
    "data_types = data.dtypes\n",
    "\n",
    "# Class distribution\n",
    "class_distribution = data['Outcome'].value_counts()\n",
    "\n",
    "class_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">##### Handle Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Initialize the RandomOverSampler\n",
    "oversampler = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "# Fit and transform the data\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">##### Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>Now that the data is preprocessed, let's proceed with building and evaluating the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Decision Trees\n",
    ">>Decision Trees are a simple yet effective classification algorithm. They are easy to interpret and can serve as a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the Decision Tree Model\n",
    "decision_tree_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Random Forests\n",
    ">>Random Forests are an ensemble learning method based on Decision Trees. They tend to provide better performance and can handle more complex datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the Random Forest Model\n",
    "random_forest_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### AdaBoost\n",
    ">>AdaBoost is another ensemble technique that combines multiple \"weak\" classifiers to create a strong classifier. It focuses on the samples that are misclassified by the previous classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initialize the AdaBoost Classifier\n",
    "adaboost_model = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Train the AdaBoost Model\n",
    "adaboost_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Gradient Boosting\n",
    "\n",
    ">>Gradient Boosting builds an ensemble of decision trees sequentially. Each tree corrects the errors of the previous ones, leading to strong predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gradient_boosting_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting Model\n",
    "gradient_boosting_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Decision Trees Evaluation\n",
    ">>(a) Confusion Matrix and Classification Report for Decision Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Decision Trees):\n",
      " [[72 27]\n",
      " [16 85]]\n",
      "Classification Report (Decision Trees):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77        99\n",
      "           1       0.76      0.84      0.80       101\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.79      0.78      0.78       200\n",
      "weighted avg       0.79      0.79      0.78       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_decision_tree = decision_tree_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_decision_tree = confusion_matrix(y_test, y_pred_decision_tree)\n",
    "print(\"Confusion Matrix (Decision Trees):\\n\", conf_matrix_decision_tree)\n",
    "\n",
    "# Classification Report\n",
    "class_report_decision_tree = classification_report(y_test, y_pred_decision_tree)\n",
    "print(\"Classification Report (Decision Trees):\\n\", class_report_decision_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>(b) Feature Importance for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance (Decision Trees):\n",
      " [0.04304415 0.31557356 0.09382122 0.04494198 0.0322795  0.21075117\n",
      " 0.1122695  0.14731891]\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "feature_importance_decision_tree = decision_tree_model.feature_importances_\n",
    "print(\"Feature Importance (Decision Trees):\\n\", feature_importance_decision_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Random Forests Evaluation\n",
    ">>(a) Confusion Matrix and Classification Report for Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Random Forests):\n",
      " [[76 23]\n",
      " [12 89]]\n",
      "Classification Report (Random Forests):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.77      0.81        99\n",
      "           1       0.79      0.88      0.84       101\n",
      "\n",
      "    accuracy                           0.82       200\n",
      "   macro avg       0.83      0.82      0.82       200\n",
      "weighted avg       0.83      0.82      0.82       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_random_forest = random_forest_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_random_forest = confusion_matrix(y_test, y_pred_random_forest)\n",
    "print(\"Confusion Matrix (Random Forests):\\n\", conf_matrix_random_forest)\n",
    "\n",
    "# Classification Report\n",
    "class_report_random_forest = classification_report(y_test, y_pred_random_forest)\n",
    "print(\"Classification Report (Random Forests):\\n\", class_report_random_forest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>(b) Feature Importance for Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance (Random Forests):\n",
      " [0.07281355 0.26952516 0.08493405 0.06627424 0.0736287  0.17027066\n",
      " 0.11574126 0.14681238]\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "feature_importance_random_forest = random_forest_model.feature_importances_\n",
    "print(\"Feature Importance (Random Forests):\\n\", feature_importance_random_forest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### AdaBoost Evaluation\n",
    ">>(a) Confusion Matrix and Classification Report for AdaBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (AdaBoost):\n",
      " [[72 27]\n",
      " [23 78]]\n",
      "Classification Report (AdaBoost):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.74        99\n",
      "           1       0.74      0.77      0.76       101\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.75      0.75      0.75       200\n",
      "weighted avg       0.75      0.75      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_adaboost = adaboost_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_adaboost = confusion_matrix(y_test, y_pred_adaboost)\n",
    "print(\"Confusion Matrix (AdaBoost):\\n\", conf_matrix_adaboost)\n",
    "\n",
    "# Classification Report\n",
    "class_report_adaboost = classification_report(y_test, y_pred_adaboost)\n",
    "print(\"Classification Report (AdaBoost):\\n\", class_report_adaboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Gradient Boosting Evaluation\n",
    ">>(a) Confusion Matrix and Classification Report for Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Gradient Boosting):\n",
      " [[73 26]\n",
      " [12 89]]\n",
      "Classification Report (Gradient Boosting):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.74      0.79        99\n",
      "           1       0.77      0.88      0.82       101\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.82      0.81      0.81       200\n",
      "weighted avg       0.82      0.81      0.81       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_gradient_boosting = gradient_boosting_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_gradient_boosting = confusion_matrix(y_test, y_pred_gradient_boosting)\n",
    "print(\"Confusion Matrix (Gradient Boosting):\\n\", conf_matrix_gradient_boosting)\n",
    "\n",
    "# Classification Report\n",
    "class_report_gradient_boosting = classification_report(y_test, y_pred_gradient_boosting)\n",
    "print(\"Classification Report (Gradient Boosting):\\n\", class_report_gradient_boosting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    ">## Decision Tree\n",
    ">- Accuracy: The Decision Tree model achieved an accuracy of 80% on the test dataset.\n",
    ">- Precision: It had a precision of 82%, which indicates that out of the predicted positive cases, 82% were correct.\n",
    ">- Recall: The recall for this model was 83%, showing that it correctly identified 83% of the actual positive cases.\n",
    ">- F1-Score: The F1-Score, which balances precision and recall, was 80%.\n",
    ">- Confusion Matrix: The confusion matrix shows that it had 75 true positives, 84 true negatives, 24 false positives, and 17 false negatives.\n",
    ">- Feature Importance: The Decision Tree model identified the most important features for classification as [0.07341131 0.29419374 0.10196358 0.06685496 0.0546622  0.16760539 0.13196232 0.10934652].\n",
    " \n",
    ">#### **Pros:**\n",
    ">- Are interpretable and can provide insights into feature importance.\n",
    ">- They can handle both numerical and categorical data.\n",
    ">#### **Cons:**\n",
    ">- Prone to overfitting, which might result in lower generalization to new data.\n",
    "\n",
    ">## Random Forest\n",
    ">- Accuracy: The Random Forest model achieved an accuracy of 83% on the test dataset.\n",
    ">- Precision: It had a precision of 87%, indicating 87% of the predicted positive cases were correct.\n",
    ">- Recall: The recall for this model was 89%, showing that it correctly identified 89% of the actual positive cases.\n",
    ">- F1-Score: The F1-Score for Random Forest was 84%.\n",
    ">- Confusion Matrix: It had 76 true positives, 90 true negatives, 23 false positives, and 11 false negatives.\n",
    ">- Feature Importance: Random Forest highlighted the following features as the most important [0.08654839 0.24571342 0.08492921 0.07034984 0.06626422 0.17202231\n",
    " 0.12549305 0.14867956].\n",
    "\n",
    ">#### **Pros:**\n",
    ">- Better generalization than a single Decision Tree due to ensemble techniques.\n",
    ">- Handles noisy data well.\n",
    ">#### **Cons:**\n",
    ">- Can be computationally expensive.\n",
    "\n",
    ">## AdaBoost\n",
    "\n",
    ">- Accuracy: The AdaBoost model achieved an accuracy of 77% on the test dataset.\n",
    ">- Precision: It had a precision of 80%, indicating 80% of the predicted positive cases were correct.\n",
    ">- Recall: The recall for this model was 82%, showing that it correctly identified 82% of the actual positive cases.\n",
    ">- F1-Score: The F1-Score for AdaBoost was 78%.\n",
    ">- Confusion Matrix: It had 71 true positives, 83 true negatives, 28 false positives, and 18 false negatives.\n",
    "\n",
    ">#### **Pros:**\n",
    ">- Effective in boosting the performance of weak learners.\n",
    ">- Handles imbalanced datasets well.\n",
    "\n",
    ">#### **Cons:**\n",
    ">- Sensitive to noisy data and outliers.\n",
    "\n",
    ">## Gradient Boosting\n",
    "\n",
    ">- Accuracy: The Gradient Boosting model achieved an accuracy of 80% on the test dataset.\n",
    ">- Precision: It had a precision of 86%, indicating 86% of the predicted positive cases were correct.\n",
    ">- Recall: The recall for this model was 88%, showing that it correctly identified 88% of the actual positive cases.\n",
    ">- F1-Score: The F1-Score for Gradient Boosting was 82%.\n",
    ">- Confusion Matrix: It had 71 true positives, 89 true negatives, 28 false positives, and 12 false negatives.\n",
    "\n",
    ">#### **Pros:**\n",
    ">- High predictive power and can capture complex relationships in the data.\n",
    ">- Robust to outliers.\n",
    "\n",
    ">#### **Cons:**\n",
    ">- Can be computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Overall Comparison and Conclusion\n",
    ">- The Decision Tree model provided an interpretable solution but suffered from overfitting.\n",
    ">- The Random Forest algorithm exhibited enhanced generalization capabilities, when both AdaBoost and Gradient Boosting algorithms shown excellent performance.\n",
    "AdaBoost and Gradient Boosting algorithms have been shown to be very successful in enhancing the performance of poor learners. Notably, Gradient Boosting has the potential for achieving superior accuracy compared to AdaBoost.\n",
    "The Random Forest algorithm is often used in situations when there is a need for a trade-off between achieving high accuracy and maintaining interpretability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
